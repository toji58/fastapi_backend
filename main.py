{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": None,
      "metadata": {
        "colab": {
          "background_save": True,
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRQ3nBefh6iM",
        "outputId": "87145680-f975-4e33-ab17-9359f8a4637f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "âœ… Loaded gis_dataset.json, Entries: 63\n",
            "âœ… Loaded conversational_dataset.json, Entries: 105\n",
            "âœ… Loaded library_dataset.json, Entries: 33\n",
            "âœ… Loaded office_locations.json, Entries: 46\n",
            "âœ… Total Entries Loaded: 247\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "<ipython-input-1-5f4ea7526c31>:83: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
            "<ipython-input-1-5f4ea7526c31>:84: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vector_db = Chroma(embedding_function=embedding_model, persist_directory=chroma_db_path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… All datasets added to ChromaDB!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [3315]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Public API URL: https://8d96-34-75-174-68.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import torch\n",
        "import re\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Initialize FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "# Allow requests from your frontend\n",
        "origins = [\n",
        "    \"https://generative-info-system.web.app\",\n",
        "    \"http://localhost:3000\",  # Optional: For local testing\n",
        "]\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=origins,\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Define Paths (For Render deployment, use relative paths)\n",
        "dataset_folder = \"./DATASETS\"  # Ensure this folder is committed to GitHub\n",
        "chroma_db_path = \"./chroma_db\"  # This will persist data in Render\n",
        "\n",
        "# Load Datasets (Ensure these files are in the DATASETS folder in the repo)\n",
        "dataset_files = [\n",
        "    \"gis_dataset.json\",\n",
        "    \"conversational_dataset.json\",\n",
        "    \"library_dataset.json\",\n",
        "    \"office_locations.json\"\n",
        "]\n",
        "\n",
        "datasets = []\n",
        "for file in dataset_files:\n",
        "    dataset_path = os.path.join(dataset_folder, file)\n",
        "    try:\n",
        "        with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            dataset = json.load(f)\n",
        "            datasets.extend(dataset)\n",
        "        print(f\"Loaded {file}, Entries: {len(dataset)}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Dataset file '{file}' not found.\")\n",
        "\n",
        "print(f\"Total Entries Loaded: {len(datasets)}\")\n",
        "\n",
        "# Load Language Model\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\").to(device)\n",
        "\n",
        "# Load ChromaDB (Make sure the Chroma embeddings are stored in persistent directory)\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vector_db = Chroma(embedding_function=embedding_model, persist_directory=chroma_db_path)\n",
        "\n",
        "# Add datasets to ChromaDB\n",
        "documents = []\n",
        "metadata = []\n",
        "for entry in datasets:\n",
        "    if \"input\" in entry and \"output\" in entry:\n",
        "        documents.append(f\"Q: {entry['input']} A: {entry['output']}\")\n",
        "        metadata.append({\"source\": \"dataset\"})\n",
        "\n",
        "vector_db.add_texts(documents, metadatas=metadata)\n",
        "print(\"All datasets added to ChromaDB!\")\n",
        "\n",
        "# Initialize Chatbot Conversation\n",
        "def initialize_conversation():\n",
        "    system_message = \"\"\"\n",
        "    You are Santi, the AI assistant for the University of Antique.\n",
        "    Your role is to provide accurate information about UA, including courses, history, enrollment, and services.\n",
        "    Always give direct, relevant answers based on UA's official information.\n",
        "    \"\"\"\n",
        "    return [{\"role\": \"system\", \"content\": system_message}]\n",
        "\n",
        "class QueryInput(BaseModel):\n",
        "    query: str\n",
        "\n",
        "@app.post(\"/ask\")\n",
        "async def ask_model(data: QueryInput):\n",
        "    query = data.query\n",
        "    results = vector_db.similarity_search(query, k=5)\n",
        "\n",
        "    if not results:\n",
        "        return {\"response\": \"I couldn't find a relevant answer. Please contact the university for official information.\"}\n",
        "\n",
        "    context = \"\\n\".join([doc.page_content for doc in results])\n",
        "    \n",
        "    if len(context.split()) < 20:\n",
        "        return {\"response\": \"Sorry, I couldn't find enough information.\"}\n",
        "\n",
        "    prompt = f\"\"\"You are Santi, the University of Antique AI Chatbot.\n",
        "    Use the provided context to answer the question accurately.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {query}\n",
        "\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "    token_length = len(query.split())\n",
        "    max_tokens = min(50 + (token_length * 2), 250)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    answer_match = re.search(r\"Answer:\\s*(.*?)(?:\\n|$)\", response, re.DOTALL)\n",
        "    final_response = answer_match.group(1).strip() if answer_match else response\n",
        "\n",
        "    return {\"response\": final_response}\n",
        "    \n",
        "# Run FastAPI\n",
        "import uvicorn\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
