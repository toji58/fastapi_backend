{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRQ3nBefh6iM",
        "outputId": "87145680-f975-4e33-ab17-9359f8a4637f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Loaded gis_dataset.json, Entries: 63\n",
            "‚úÖ Loaded conversational_dataset.json, Entries: 105\n",
            "‚úÖ Loaded library_dataset.json, Entries: 33\n",
            "‚úÖ Loaded office_locations.json, Entries: 46\n",
            "‚úÖ Total Entries Loaded: 247\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "<ipython-input-1-5f4ea7526c31>:83: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
            "<ipython-input-1-5f4ea7526c31>:84: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vector_db = Chroma(embedding_function=embedding_model, persist_directory=chroma_db_path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All datasets added to ChromaDB!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [3315]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Public API URL: https://8d96-34-75-174-68.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import torch\n",
        "import re\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# ‚úÖ Google Colab Check\n",
        "IN_COLAB = \"google.colab\" in str(get_ipython())\n",
        "\n",
        "# ‚úÖ Initialize FastAPI (Define 'app' First)\n",
        "app = FastAPI()\n",
        "\n",
        "# ‚úÖ Allow requests from your frontend\n",
        "origins = [\n",
        "    \"https://generative-info-system.web.app\",\n",
        "    \"http://localhost:3000\",  # (Optional: For local testing)\n",
        "]\n",
        "\n",
        "# ‚úÖ Define Paths\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    dataset_folder = \"/content/drive/My Drive/DATASETS\"\n",
        "    chroma_db_path = \"/content/drive/My Drive/chroma_db\"\n",
        "else:\n",
        "    dataset_folder = \"./DATASETS\"\n",
        "    chroma_db_path = \"./chroma_db\"\n",
        "\n",
        "# ‚úÖ Load Multiple Datasets\n",
        "dataset_files = [\n",
        "    \"gis_dataset.json\",\n",
        "    \"conversational_dataset.json\",\n",
        "    \"library_dataset.json\",\n",
        "    \"office_locations.json\"\n",
        "]\n",
        "\n",
        "datasets = []\n",
        "for file in dataset_files:\n",
        "    dataset_path = os.path.join(dataset_folder, file)\n",
        "    try:\n",
        "        with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            dataset = json.load(f)\n",
        "            datasets.extend(dataset)  # ‚úÖ Append all datasets into one list\n",
        "        print(f\"‚úÖ Loaded {file}, Entries: {len(dataset)}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Error: Dataset file '{file}' not found.\")\n",
        "\n",
        "print(f\"‚úÖ Total Entries Loaded: {len(datasets)}\")\n",
        "\n",
        "# Allow requests from your frontend\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=origins,\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# ‚úÖ Fix for Colab's event loop\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ‚úÖ Setup ngrok (Replace Token)\n",
        "NGROK_AUTH_TOKEN = \"2tNiI33TgOCzv7votp0JxpAeE4u_6snykGnd8yDG3QkzqwvZS\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# ‚úÖ Load Language Model\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# ‚úÖ Ensure correct device selection\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\").to(device)\n",
        "\n",
        "# ‚úÖ Load ChromaDB (Vector Store)\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vector_db = Chroma(embedding_function=embedding_model, persist_directory=chroma_db_path)\n",
        "\n",
        "# ‚úÖ Inject Multiple Datasets into ChromaDB (Improved)\n",
        "documents = []\n",
        "metadata = []\n",
        "for entry in datasets:\n",
        "    if \"input\" in entry and \"output\" in entry:  # Ensure valid format\n",
        "        documents.append(f\"Q: {entry['input']} A: {entry['output']}\")\n",
        "        metadata.append({\"source\": \"dataset\"})\n",
        "\n",
        "vector_db.add_texts(documents, metadatas=metadata)\n",
        "print(\"‚úÖ All datasets added to ChromaDB!\")\n",
        "\n",
        "# ‚úÖ Initialize Chatbot Conversation\n",
        "def initialize_conversation():\n",
        "    \"\"\"Set system message for Santi, the UA chatbot.\"\"\"\n",
        "    system_message = \"\"\"\n",
        "    You are Santi, the AI assistant for the University of Antique.\n",
        "    Your role is to provide accurate information about UA, including courses, history, enrollment, and services.\n",
        "    Always give direct, relevant answers based on UA's official information.\n",
        "    \"\"\"\n",
        "    return [{\"role\": \"system\", \"content\": system_message}]\n",
        "\n",
        "# ‚úÖ Define Input Schema\n",
        "class QueryInput(BaseModel):\n",
        "    query: str\n",
        "\n",
        "@app.post(\"/ask\")\n",
        "async def ask_model(data: QueryInput):\n",
        "    query = data.query\n",
        "    results = vector_db.similarity_search(query, k=5)  # Increased top-k to 3 for better accuracy\n",
        "\n",
        "    # ‚úÖ Handle empty search results\n",
        "    if not results:\n",
        "        return {\"response\": \"I couldn't find a relevant answer in the database. Please contact the university for official information.\"}\n",
        "\n",
        "    context = \"\\n\".join([doc.page_content for doc in results])\n",
        "\n",
        "    # ‚úÖ Confidence Threshold\n",
        "    if len(context.split()) < 20:  # If retrieved text is too short, discard it\n",
        "        return {\"response\": \"I'm sorry, but I couldn't find enough information on that topic.\"}\n",
        "\n",
        "    prompt = f\"\"\"You are Santi, the University of Antique AI Chatbot.\n",
        "    Use the provided context to answer the question accurately and concisely.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {query}\n",
        "\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "    token_length = len(query.split())  # Count words in query\n",
        "    max_tokens = min(50 + (token_length * 2), 250)  # Dynamically adjust response length\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,  # Adjusted dynamically\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    # ‚úÖ Extract answer properly\n",
        "    answer_match = re.search(r\"Answer:\\s*(.*?)(?:\\n|$)\", response, re.DOTALL)\n",
        "    final_response = answer_match.group(1).strip() if answer_match else response\n",
        "\n",
        "    return {\"response\": final_response}\n",
        "\n",
        "\n",
        "# ‚úÖ Start ngrok tunnel\n",
        "public_url = ngrok.connect(8000).public_url\n",
        "print(f\"üöÄ Public API URL: {public_url}\")\n",
        "\n",
        "# ‚úÖ Run FastAPI in Colab\n",
        "import uvicorn\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(uvicorn.run(app, host=\"0.0.0.0\", port=8000))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
